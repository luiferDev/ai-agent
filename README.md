# ğŸ¤– AI Research Agent

<div align="center">

![AI Agent](https://img.shields.io/badge/AI-Agent-blue?style=for-the-badge&logo=robot)
![Python](https://img.shields.io/badge/Python-3.13-green?style=for-the-badge&logo=python)
![Docker](https://img.shields.io/badge/Docker-Compose-blue?style=for-the-badge&logo=docker)
![LangChain](https://img.shields.io/badge/LangChain-Framework-orange?style=for-the-badge)
![Ollama](https://img.shields.io/badge/Ollama-Local_LLM-purple?style=for-the-badge)

</div>

```
    ğŸ” Research Agent
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚  ğŸ§  AI Brain    â”‚ â”€â”€â–º ğŸŒ Web Search
   â”‚  ğŸ“š Knowledge   â”‚ â”€â”€â–º ğŸ“– Wikipedia  
   â”‚  ğŸ’¾ Memory      â”‚ â”€â”€â–º ğŸ’¾ File Save
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“‹ Description

An intelligent research agent that uses **Ollama** as a local language model, **LangChain** as framework, and multiple tools to perform comprehensive and structured research.

### âœ¨ Features

- ğŸ¤– **Local AI**: Uses Ollama for local processing without sending data externally
- ğŸ” **Web Search**: DuckDuckGo integration for real-time searches
- ğŸ“š **Wikipedia**: Direct access to the world's largest encyclopedia
- ğŸ’¾ **Persistence**: Automatically saves research results
- ğŸ³ **Docker**: Complete deployment with Docker Compose
- ğŸŒ **Nginx**: Web server for HTTP access

## ğŸš€ Quick Installation

### Prerequisites

- ğŸ³ Docker & Docker Compose
- ğŸ Python 3.13+ (for local development)

### ğŸ³ With Docker (Recommended)

```bash
# 1. Clone the repository
git clone <your-repository>
cd ai-agent

# 2. Configure environment variables
cp .env.example .env
# Edit .env with your API keys (optional for Ollama)

# 3. Start all services
docker-compose up -d

# 4. Download model in Ollama (first time)
docker exec -it ollama ollama pull granite4:350m
```

### ğŸ Local Installation

```bash
# 1. Create virtual environment
python -m venv venv
source venv/bin/activate  # Linux/Mac
# venv\Scripts\activate   # Windows

# 2. Install dependencies
pip install -r requirements.txt

# 3. Configure environment variables
cp .env.example .env

# 4. Run
python main.py
```

## ğŸ¯ Usage

### With Docker

Once services are up, the agent will be available at:
- **Application**: `http://localhost:80` (via Nginx)
- **Ollama API**: `http://localhost:11434`

### Usage Example

```python
# The agent will respond in structured format:
{
    "topic": "How to make pancakes",
    "summary": "Pancakes are...",
    "sources": ["https://example.com", "Wikipedia"],
    "tools_used": ["web_search", "wikipedia_search"]
}
```

## ğŸ› ï¸ Available Tools

| Tool | Description | Usage |
|------|-------------|-------|
| ğŸ” **Web Search** | DuckDuckGo search | Updated information |
| ğŸ“š **Wikipedia** | Encyclopedia queries | Verified knowledge |
| ğŸ’¾ **File Save** | Automatic saving | Result persistence |

## ğŸ—ï¸ Architecture

```mermaid
graph TB
    A[ğŸ‘¤ User] --> B[ğŸŒ Nginx]
    B --> C[ğŸ¤– AI Agent]
    C --> D[ğŸ§  Ollama LLM]
    C --> E[ğŸ” DuckDuckGo]
    C --> F[ğŸ“š Wikipedia]
    C --> G[ğŸ’¾ File Storage]
```

## ğŸ“ Project Structure

```
ai-agent/
â”œâ”€â”€ ğŸ³ compose.yml          # Docker Compose
â”œâ”€â”€ ğŸ³ Dockerfile           # App image
â”œâ”€â”€ ğŸŒ nginx.conf           # Nginx configuration
â”œâ”€â”€ ğŸ¤– main.py              # Main application
â”œâ”€â”€ ğŸ› ï¸ tools.py             # Agent tools
â”œâ”€â”€ ğŸ“‹ requirements.txt     # Python dependencies
â”œâ”€â”€ ğŸ”’ .env.example         # Environment variables
â”œâ”€â”€ ğŸš« .gitignore          # Ignored files
â””â”€â”€ ğŸ“– README.md           # This file
```

## âš™ï¸ Configuration

### Environment Variables

```bash
# .env
ANTHROPIC_API_KEY="your-api-key-here"  # Optional
OPENAI_API_KEY="your-api-key-here"     # Optional
```

### Available Ollama Models

```bash
# Recommended models
ollama pull granite4:350m    # Fast and efficient
ollama pull llama2          # General model
ollama pull codellama       # Code specialized
```

## ğŸ”§ Development

### Adding New Tools

```python
# tools.py
@tool
def new_tool(parameter):
    """Tool description"""
    # Your logic here
    return result
```

### Customize the Agent

```python
# main.py
agent = create_agent(
    model=llm,
    tools=[new_tool],  # Add here
    system_prompt="Your custom prompt"
)
```

## ğŸ› Troubleshooting

### Common Issues

| Problem | Solution |
|---------|----------|
| ğŸš« Ollama not responding | `docker-compose restart ollama` |
| ğŸ“¦ Model not found | `docker exec -it ollama ollama pull <model>` |
| ğŸŒ Connection error | Check that all services are running |

### Logs

```bash
# View logs from all services
docker-compose logs -f

# View specific logs
docker-compose logs -f ai-agent
docker-compose logs -f ollama
docker-compose logs -f nginx
```

## ğŸ¤ Contributing

1. ğŸ´ Fork the project
2. ğŸŒ¿ Create a branch (`git checkout -b feature/new-feature`)
3. ğŸ’¾ Commit your changes (`git commit -m 'Add new feature'`)
4. ğŸ“¤ Push to the branch (`git push origin feature/new-feature`)
5. ğŸ”„ Open a Pull Request

## ğŸ“„ License

This project is under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- ğŸ¦™ [Ollama](https://ollama.ai/) - Local LLM
- ğŸ”— [LangChain](https://langchain.com/) - AI Framework
- ğŸ³ [Docker](https://docker.com/) - Containerization
- ğŸŒ [Nginx](https://nginx.org/) - Web Server

---

<div align="center">

**Made with â¤ï¸ and lots of â˜•!**

![Stars](https://img.shields.io/github/stars/yourusername/ai-agent?style=social)
![Forks](https://img.shields.io/github/forks/yourusername/ai-agent?style=social)

</div>